{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9461ede-b3ec-4c5e-a182-413360caa8ed",
   "metadata": {},
   "source": [
    "### This is an [example](https://tomekkorbak.com/2020/06/26/implementing-attention-in-pytorch/) of the attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957850c5-3347-4429-9dc2-cd413f9e6e83",
   "metadata": {},
   "source": [
    "####  First, the LSTM layer.\n",
    "![alt text](https://miro.medium.com/max/1400/1*IM5fjlTYrdYD5XAq2dVEvQ.png  \"LSTM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be1e4536-c911-4089-898e-dbf2255978c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4513b18f-a47a-4022-8d5f-a0013194a386",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\" use seq of 2\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, bidirectional=True):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, bidirectional=bidirectional)\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "\n",
    "        output, hidden = self.lstm(inputs.view(-1, 1, self.input_size), hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(1 + int(self.bidirectional), 1, self.hidden_size),\n",
    "          torch.zeros(1 + int(self.bidirectional), 1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f810bb7-61ce-4658-87f9-a6d22d7426ac",
   "metadata": {},
   "source": [
    "### The attention layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac2cee3-0d8b-4dc2-947a-20ad2062b9a1",
   "metadata": {},
   "source": [
    "Self attention, note Key and Value are same. The formula is corresponding to the MultiplicativeAttention\n",
    "<div>\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20200812212119/encoderselfattention.PNG\" width=\"600\"/>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0ab0f13-b5c0-4633-90f2-bcd2521f0bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    use values as key. Do not use sample dimension\n",
    "    query -- decoder; \n",
    "    value -- encoder;\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_dim: int, decoder_dim: int):\n",
    "        super().__init__()\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "\n",
    "    def forward(self, \n",
    "        query: torch.Tensor,  # [decoder_dim]\n",
    "        values: torch.Tensor, # [seq_length, encoder_dim]\n",
    "        ):\n",
    "        weights = self._get_weights(query, values) # [seq_length]\n",
    "        weights = torch.nn.functional.softmax(weights, dim=0)\n",
    "        return weights @ values  # [encoder_dim]\n",
    "\n",
    "    \n",
    "class AdditiveAttention(Attention):\n",
    "\n",
    "    def __init__(self, encoder_dim, decoder_dim):\n",
    "        super().__init__(encoder_dim, decoder_dim)\n",
    "        self.v = torch.nn.Parameter(\n",
    "            torch.FloatTensor(self.decoder_dim).uniform_(-0.1, 0.1))\n",
    "        self.W_1 = torch.nn.Linear(self.decoder_dim, self.decoder_dim)\n",
    "        self.W_2 = torch.nn.Linear(self.encoder_dim, self.decoder_dim)\n",
    "\n",
    "    def _get_weights(self,        \n",
    "        query: torch.Tensor,  # [decoder_dim]\n",
    "        values: torch.Tensor,  # [seq_length, encoder_dim]\n",
    "    ):\n",
    "        query = query.repeat(values.size(0), 1)  # [seq_length, decoder_dim]\n",
    "        weights = self.W_1(query) \n",
    "        weights += self.W_2(values)  # [seq_length, decoder_dim]\n",
    "        print(f\"{weights.shape}--weight\")\n",
    "        return torch.tanh(weights) @ self.v  # [seq_length]\n",
    "    \n",
    "\n",
    "class MultiplicativeAttention(Attention):\n",
    "\n",
    "    def __init__(self, encoder_dim: int, decoder_dim: int):\n",
    "        super().__init__(encoder_dim, decoder_dim)\n",
    "        self.W = torch.nn.Parameter(torch.FloatTensor(\n",
    "            self.decoder_dim, self.encoder_dim).uniform_(-0.1, 0.1))\n",
    "\n",
    "    def _get_weights(self,\n",
    "        query: torch.Tensor,  # [decoder_dim]\n",
    "        values: torch.Tensor, # [seq_length, encoder_dim]\n",
    "    ):\n",
    "        weights = query @ self.W @ values.T  # [seq_length]\n",
    "        return weights/np.sqrt(self.decoder_dim)  # [seq_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab346c5-71ef-46fd-9a7e-3c9dea85a0cf",
   "metadata": {},
   "source": [
    "#### Let's review the dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8762e2da-6feb-4f1f-8f69-172af71eaa70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 16])--a.shape\n",
      "torch.Size([2, 1, 8])--b[0].shape\n",
      "torch.Size([2, 1, 8])--b[1].shape\n",
      "torch.Size([1, 16])--w.shape\n"
     ]
    }
   ],
   "source": [
    "bidirectional = True\n",
    "\n",
    "input_dim = 5\n",
    "output_dim = 16\n",
    "hidden_dim = 8\n",
    "\n",
    "\n",
    "\n",
    "c = Encoder(input_dim, hidden_dim, bidirectional)\n",
    "a, b = c.forward(torch.randn(10), c.init_hidden())\n",
    "print(f\"{a.shape}--a.shape\")\n",
    "print(f\"{b[0].shape}--b[0].shape\")\n",
    "print(f\"{b[1].shape}--b[1].shape\")\n",
    "\n",
    "#x = AdditiveAttention(encoder_dim=16, decoder_dim=16)\n",
    "x = MultiplicativeAttention(encoder_dim=16, decoder_dim=16)\n",
    "y= x.forward(query=a[0], values=a.squeeze()) \n",
    "\n",
    "\n",
    "print(f\"{y.shape}--w.shape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e62b56-8953-4b22-84f6-3bee65a2118d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
